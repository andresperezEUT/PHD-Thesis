\chapter{Data generation and storage}



Explain about mono files plus ambisonics IRs.


\section{Recorded IRs}


Impulse Responses (IRs) measurements constitute a compact way of representing the acoustic propertiesof a linear time-invariant system. When such measurements are performed in a specific room or enclosure, the so-called Room Impulse Responses (RIRs) are able to capture the intrinsic reverberation and acoustic characteristics of the enclosure, for which several methods have been developed these past years \cite{stan2002comparison}. Furthermore, it is possible to account for different emitter/receiver positions in the measurement, usually performing the measurement with a microphone array. In that case, this kind of measurements is referred to as Directional Room Impulse Responses (DRIRs) \cite{embrechts2005computation}. DRIRs have a wide range of applications: auralization \cite{embrechts2005computation}, room acoustics analysis \cite{embrechts2015measurement,clapp2011investigations} and modelling \cite{romblom2017diffuse}, spatial audio synthesis \cite{coleman2017object}, source separation and dereverberation \cite{baque2016separation}, acoustic heritage preservation \cite{gerzon1975recording,murphy2005multi}, etc. 


\subsection{Ambisonics Recording}

It is also possible to capture Ambisonics audio scenes by using specific recording devices. Spherical microphone arrays (also known as \textit{Ambisonics microphones}) are a type of microphone arrays in which the capsules are located around a spherical surface, presenting rotational symmetry. Such geometric arrangement allows the recorded signals to be transformed to the Ambisonics domain, by means of projection into the Spherical Harmonics basis, and further equalization with radial filters \cite{Bertet2006}.

This process is known in the audio production domain as \textit{A-B Conversion}. All major spherical microphone array manufacturers provide tools for achieving this transformation \cite{soundfield,ambeo,zylia,tetra,em32}.


\subsection{Ambisonics DRIRs}

The intrinsic spatial capabilities of Ambisonics microphones might be applied to DRIR recording, as originally proposed by Gerzon in the context of acoustical heritage preservation \cite{gerzon1975recording}. In recent years, several datasets of Ambisonics DRIRs have been publicly released, such as the OpenAIR database \cite{murphy2010openair} or the set of  measurements performed in the scope of the S3A project \cite{coleman2015s3a,openair}. 

\subsection{The SOFA Conventions}

In general, the Ambisonics DRIR databases show a common approach for describing the measurements: given a specific room, usually represented as a folder, IR data consist of several multichannel audio files, with one audio channel per spherical harmonic, and one file per emitter/receiver combination. Furthermore, it is also usual to provide a \textit{metadata} file, describing the different emitter and receiver positions, and eventually some information about the measurement setup, methodology, etc. Such files might be formatted as plain text or delimiter-separated value files.

Despite the common approach, it can be foreseen that each database generated by a different individual or institution might potentially have a different naming convention, folder structure, file format, and so on. This situation hinders data manipulation and exchange, and forces users to write ad-hoc parsers and algorithms for each specific database.

The \textit{Spatially Oriented Format for Acoustics} (SOFA) is a file format designed for a consistent, standardized storage and manipulation of IR data \cite{majdak2015aes69}. The need for such standard arose from dealing with different databases of \textit{Head-Related Transfer Functions} (HRTFs), in a similar manner as the one mentioned for Ambisonics DRIRs.

There are several SOFA conventions, each one addressing a particular type of IR measurement. In the case of Ambisonics DRIRs, given the data representation in existing databases, one could outline the following specificities:
 
\begin{itemize}
    \item Presence of Ambisonics-related information (Ambisonics order, channel ordering and normalization)
    \item Audio stored in the Ambisonics (spherical harmonics) domain  
    \item Data structure support for different combinations of source and receiver positions
\end{itemize}


However, none of the existing SOFA conventions meets those requisites. The potentially first candidate by name, \textit{SingleRoomDRIR}, is limited to one source position per file. On the other hand, \textit{MultiSpeakerBRIR} allows for multiple sound sources, but restricts the number of receivers (microphone capsules) to two, as expected in a binaural recording. \textit{GeneralFIRE} is intended for "data which are too general to store in more specific conventions" \cite{generalfire}.


\subsection{Convention Proposal}

\subsubsection{Considerations}

The SOFA specification defines some criteria that must be fulfilled in order to propose a new convention \cite{sofaconventions}. These criteria are:
\begin{enumerate}
    \item Data must exist.
    \item Data can not be described by existing SOFA conventions.
    \item Relevant information about the data must be available.
\end{enumerate}


As we already mentioned, existing databases of Ambisonics DRIRs can be found in OpenAIR and S3A databases (criterium 1). Criterium 2 has been discussed in the previous paragraph. All available Ambisonics DRIR datasets are accompanied by explanations, pictures, diagrams and related information (criterium 3). 

\subsubsection{Specifications}

We therefore propose a new SOFA convention, \textit{AmbisonicsDRIR}, designed for DRIRs measured with spherical microphone arrays, and presented in the Ambisonics domain. In other words, we propose to store the instantaneous $B_{mn}$ values for a given Ambisonics order $L$, provided that $S$ is a unit impulse (Eq. \ref{eq1}).

The \textit{Listener} - as defined by the SOFA specification - is embodied by the spherical microphone array. The different \textit{Receivers} are the different Ambisonics Components $B_{mn}$, so that their number is fixed provided $L$ (more precisely, by following the relationship $R = (L+1)^2$), and their positions are not applicable. Furthermore, there might be many different \textit{Sources}, all of them omnidirectional and consisting of one only \textit{Emitter}. In that sense, \textit{M} represents the number of different \textit{Source} positions. 


The proposed convention is based on \textit{GeneralFIRE}, with the following modifications:
\begin{itemize}

    \item Mandatory field \textit{GLOBAL:AmbisonicsOrder} (type \textit{double}, dimension \textit{I}, default \textit{1}). Indicates the order of the Spherical Harmonic expansion.
    
    \item Mandatory field \textit{DATA:ChannelOrdering} (type \textit{attribute}, default \textit{acn}). Describes the ordering of the different Ambisonics Channels. Must be one of: \textit{acn} or \textit{fuma}.
    
    \item Mandatory field \textit{DATAChannelNormalization} (type \textit{attribute}, default \textit{sn3d}). Describes the Ambisonics normalization convention used in the data. Must be one of: \textit{sn3d}, \textit{n3d}, \textit{fuma} or \textit{maxn}.
    
    \item Fields \textit{ReceiverPosition}, \textit{ReceiverPosition:Type} and \textit{ReceiverPosition:Units} are not needed, since it is assumed that \textit{Receivers} represent the Ambisonics channels. Furthermore, the values of \textit{R} (number of receivers) are defined by $R = (L+1)^2$. Accordingly, the values of \textit{GLOBAL:AmbisonicsOrder} and \textit{R} are valid only if they follow the given equation.
    
    \item Field \textit{DATA:Delay} is not mandatory.
    
    \item Relevant information about the microphone model and brand, Ambisonics encoding methodology, software used, etc, is recommended to be added into the field \textit{GLOBAL:Comment}.
    
\end{itemize}

\subsection{Results}  

As a preliminary result, we have extended the current SOFA C++  \cite{sofacpp} and Matlab \cite{sofamo} APIs to be fully compatible with the AmbisonicsDRIR convention. \todo{pysofaconventions}

Furthermore, as a use-case, a selection of existing Ambisonics DRIRs have been transcoded to the proposed convention: \textit{Emmanuel Main Church} from S3A database \cite{coleman2015s3a}, and \textit{Heslington Church} and \textit{York Guildhall Council Chamber} from the OpenAIRlib \cite{openair}. Figure \ref{guildhall} shows a schematic diagram of the different \textit{Emitter/Receiver} positions at the \textit{York Guildhall Council Chamber} recordings. The data, as well as the tools used to perform the conversion, are available online under an open source license \cite{ambisonicsdrirexamples}.

Finally, we must remark that the \textit{AmbisonicsDRIR} SOFA convention proposal is currently under discussion, and the described specifications are subject to change with upcoming versions. 

% TODO: include some examples!!! about data dimensions etc

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Figures/DataGeneration/guildhall}
	\caption{Source and Listener position diagram for the "York Guildhall Council Chamber" Ambisonics DRIR (original diagram attribution to \cite{guildhall} }
	\label{guildhall}
\end{figure}

\subsection{Summary} 

This document addresses the lack of compatibility among different databases of Ambisonics DRIRs. The present proposal consists in defining a new SOFA convention, specifically designed for Ambisonics DRIRs. That way we contribute to improve the ease of data manipulation and database interoperability. Software implementations and tools for automatic conversion are provided.



\section{Simulated IRs}

explain different methods and libraries.
tell about masp.






% ==================================================================
\section{High-level scene description}

\subsection{Motivation}

\todo{introduce the topic}

\todo{ARRANGE this introduction in a meaningful way}\\
The work by Moore \cite{Moore2015} was the first to combine Intensity Vector statistics with DPD-test preprocessing. A similar approach is found on the \textit{Single Source Zones (SSZs)} algorithm, first presented by Pavlidi \cite{Pavlidi2015}. Other recent proposals, as for example the work by He \cite{He2017}, consider the local DOA variance as an estimator of reliability. 

Source Localization based on IV statistics has been also successfully used as a preprocessing step for Source Separation tasks. The proposal of G\"unel \cite{Gunel2008}, which uses beamforming over the DOA estimation, was a pioneer work on this scope. Other similar approaches, which rely on DOA-based Time-Frequency (TF) masking, can be found on the works by Shujau \cite{Shujau2011}, Riaz \cite{Riaz2015} or Chen \cite{Chen2015}.



\subsection{Evaluation data}
\label{subsec:evaluationdata}

Audio data used for evaluation in the works presented in Section \ref{subsec:algorithms} can be classified in  three main categories:


\begin{itemize}

    \item \textit{IR simulations:} Impulse Responses simulated by numerical methods, using software tools such as \textit{SMIR Generator} \cite{smir}. IR simulation provides the most flexible approach for evaluation, since it allows custom design and parametric analysis (algorithm evaluation as a function of reverberation time, source(s) distance(s), etc). However, the resulting audio data realism is restricted by the simulation algorithm, which is usually limited to empty \textit{shoebox} rooms.
    
    \item \textit{IR recordings:} Impulse Responses (IRs) of specific rooms recorded with an Ambisonics microphone, as first presented by Gerzon in the context of acoustic heritage preservation \cite{gerzon1975recording}. IR recordings do not provide the flexibility of IR simulations, but allow realistic captures of the room's acoustic properties, while maintaining the potential of using different source contents and positions. OpenAIRlib \cite{murphy2010openair} is the reference public repository for IRs, even including a dedicated Ambisonics IR section.

    \item \textit{Audio recordings:} a sound scene recorded in a specific room, using an Ambisonics microphone and real sound sources. This approach  represents the  most  accurate  option  for algorithm evaluation in a real scenario. However, its cost and lack of flexibility makes this approach only attractive for last stages of algorithm  evaluation.

\end{itemize}

In the case of IR simulations and recordings, the Ambisonics IRs are convolved with anechoic monophonic recordings to produce the actual evaluation data. The anechoic recordings are usually taken from standard audio processing databases, such as TIMIT \cite{timit}. \\

Table \ref{table:evaluationdata} summarizes the evaluation data types used in each work review in Section \ref{subsec:algorithms}, along with the audio content type and origin database (if any).    


%% DOA ESTIMATION
\begin{table}[htpb]
    \centering
    \footnotesize

	\begin{tabular}{ p{2cm} p{2.8cm} p{2.5cm} p{2.3cm} }
    \toprule
    % \cmidrule(r){1-2}
    
    % \centering 
    \textbf{Article} &
    % \textbf{\textit{$L$}} &
    \textbf{Evaluation Data Type} &
    \textbf{Audio Content Type} &
    \textbf{Database}\\
    \midrule
    
    Thiergart \cite{Thiergart2009}
    % & 1 h
    & Audio recording 
    & Speech
    & -\\
    
    Tervo \cite{Tervo2009}
    % & 1 h
    & Audio recording
    & Noise, music
    & -\\
    
    Jarret \cite{Jarrett2010} 
    % & 4 
    & IR simulation, Audio recording
    & Noise
    & -\\
    
    Nadiri \cite{Nadiri2014}   
    % & 4 
    & IR simulation, Audio recording
    & Speech
    & TIMIT \\
    
    Moore \cite{Moore2015}   
    % & 4 
    & IR simulation 
    & Speech
    & APLAWD \\
    
    Pavlidi \cite{Pavlidi2015}
    % & 4
    & IR simulation 
    & Noise, speech
    & -\\
    
    He \cite{He2017}   
    % & 1 h
    & IR simulation, Audio recording
    & Speech
    & TIMIT \\

    
    % Ding \cite{Ding2017}   
    % % & 1 h
    % & IR simulation \& Audio recording \\
    
    \midrule
    
    Gunel \cite{Gunel2008}
    & IR recording
    & Speech, music
    & Music for Archimedes \\
    
    Shujau \cite{Shujau2011} 
    & Audio recording
    & Speech
    & TIMIT \\
    
    Riaz \cite{Riaz2015}
    & IR recording
    & Speech, music
    & Music for Archimedes \\
    
    Chen \cite{Chen2015}
    & IR simulation, IR recording
    & Speech
    & TIMIT \\

    \bottomrule
    \end{tabular}
    \caption{Summary of audio data used across Ambisonics-based Source Localization (above) and Source Separation (below)  algorithm proposals. \todo{update table? fix references}}
    \label{table:evaluationdata}
\end{table}



Although algorithm evaluation in real scenarios provides the most realistic approach to the problem, this methodology has not been widely adopted due to its cost and lack of flexibility. As an example, none of the works using real recordings in Table \ref{table:evaluationdata} performs the evaluation with more than one  recording in each case. In contrast, when using IR-based  scenes, the number of audios evaluated are usually one or two magnitude orders greater. 

However, it is important to notice the lack of public availability of evaluation data. None of the analyzed articles provide a way to access neither the used audio dataset, nor the groundtruth (position annotations in the case of Sound Localization, and original sound sources for Sound Separation). Only in the case of simulated IRs it is possible to partially replicate the experimental setup, since some of the parameters used in the simulation software are usually provided. Furthermore, the process of creation of the custom datasets (selecting an anechoic audio dataset, convolving with custom IRs, or performing real recordings) seems to be performed \textit{ad-hoc} on each article. 

Taking into account the flexibility offered by IR-based scenes, it would be desirable to have a tool for automatic generation of reverberant Ambisonics scenes (and their associated groundtruth) for analysis purposes. Such tool would help the scientific community in several ways: reducing the amount of time dedicated to build custom datasets, reusing publicly available resources and recordings, and enhancing experiment reproducibility by making easier the exchange of datasets. Furthermore, the capacity of producing a big number of diverse audio scenes for analysis will help the algorithm design and early testing stages,and specially the training stage of machine learning-based algorithms. 


\subsection{AMBISCAPER}
\label{sec:ambiscaper}

\todo{this has been downgraded to subsection. maybe place it at the introduction or something to find a more meaningful explanation}

AmbiScaper \cite{ambiscaper} is a tool designed to provide a flexible way of creating complex Ambisonics sound scenes and their associated groundtruth, to be used in the context of Source Localization and Source Separation algorithms.
AmbiScaper offers a high level control of the sound scene parameters, and provides an easy way of creating large datasets with custom characteristics.
AmbiScaper is based on Scaper, a framework designed to generate ground truth information to train Sound Event Detection models \cite{Salamon2017}.

\subsection{Sound scene description}
\label{subsec:description}

One of the main features of AmbiScaper is that all parameters of the sound scene can be specified in a non-deterministic way. In that sense, the parameters for each \textit{event} (sound source) are actually generated through a two-step process. First, in the \textit{Event Specification}, all parameters related to an event are defined in terms of statistical distributions. During the \textit{Event Instanciation}, the actual values for each parameter are then sampled from the statistical distributions.
This two-step process allows the user to describe abstract \textit{templates} of sound scenes, rather than manually assigning values to parameters. Therefore, a single \textit{event specification} might produce potentially infinite different sound scenes. 



\subsection{Architecture}
\label{subsec:architecture}

\begin{figure}
\label{fig_architecture}
  \centering
    \includegraphics[width=\textwidth]{Figures/DataGeneration/figure_architecture_V2.png}
    \caption{AmbiScaper architecture.}
\end{figure}

%%%%%%%%
In order to generate a sound scene, AmbiScaper requires three different inputs: the original \textit{mono signals}, which will be the basis for the scene's audio content, an optional \textit{Ambisonics IR}, and the \textit{event specification}. 

The process of dataset creation starts with the \textit{event instanciation}, as described in Section \ref{subsec:description}. Once all values are sampled, three different types of output are generated: the \textit{Ambisonics scene}, the \textit{instanciated mono signals} (the original mono signals after data augmentation and duration changes), and the \textit{annotations}, in the form of a JAMS file \cite{humphrey2014jams}, containing all information about the scene specification and the instanciated values. 

AmbiScaper's architecture is depicted in Figure \ref{fig_architecture}.


\subsection{Reverberation}

When no reverberation is specified, AmbiScaper can generate anechoic sound scenes, which can be useful for baseline performance evaluation. In this case, there is no upper limit on the Ambisonics order of the rendered scene. Furthermore, the anoechic case allows for the specification of source \textit{spread} or apparent size through Ambisonics order downgrade \cite{carpentier2017ambisonic}.

AmbiScaper supports the usage of recorded Ambisonics IRs, although it is currently limited to IRs from the S3A database \cite{coleman2015s3a}. The development of a standardized file format for Ambisonics IRs, which is being discussed at the moment of writing, will provide the flexibility to work with arbitrary Ambisonics IRs.   

Lastly, AmbiScaper features the possibility of using simulated Ambisonics IRs, through a wrapper to \textit{SMIR Generator}. In this case, the reverberation model specifications might be defined as well in statistical terms, and the generated IRs are stored for evaluation purposes. A working copy of Matlab is required to run this option.

When a reverberant sound scene is created, the specific Ambisonics IRs used on the scene are also provided as an output. Other research problems such as dereverberation or room reflection modelling might therefore benefit from such data.




\subsection{AmbiScaper and experiment reproducibility}

As mentioned in Section \ref{subsec:evaluationdata}, there is a generalized lack of publicly available datasets for Source Localization and Source Separation in the Ambisonics domain. Even when using general purpose audio/speech datasets (such as TIMIT), the actual reverberant Ambisonics evaluation data is usually not available.
In that sense, the compatibility of AmbiScaper with public Ambisonics IR databases is a key aspect for reproducibility, since it allows for the reutilization of acoustical measurements in a systematic way.

Furthermore, the output of the AmbiScaper dataset generation process is not limited to the actual dataset. In fact, as explained in Section \ref{subsec:architecture}, the resulting annotation file does not only contain the \textit{instanciation} (the actual values of each parameter in the sound scene), but also the \textit{specification} (the statistical distributions from which the instanciated values are sampled). In the scope of experiment reproducibility, the exchange of \textit{specification files} instead of actual audio files greatly reduces the storage capacitiy and bandwith required to transfer big databases. 

AmbiScaper is implemented in the form of a Python package, publicly available through the \textit{Python Package Index} repository. 
AmbiScaper is free software under the GPL license, easing the software adoption and the potential engagement of the scientific community with the development. 


\subsection{Sample Dataset}
\label{ssec:dataset}

As an example of the potential capabilities of AmbiScaper,  we have created and published a dataset for the evaluation of source localization algorithms \footnote{https://zenodo.org/record/1186907 \todo{link}}.

The dataset contains 300 first order Ambisonics sound scenes, each one containing from one to three static sound sources (be they simultaneous or not), with different gains and SNRs, and placed at random positions around the sphere. Each sound scene has a duration between 1 and 2 seconds.

The sources are randomly chosen from a subset of the Anechoic OpenAirlib database, which mostly contains recordings from baroque musical instruments.
The AudioBooth IR database from S3A has been used for all scenes.
It features SoundField recordings from an acoustically treated room with a geodesic dome structure, to which 17 speakers have been attached.



\subsection{CONCLUSIONS AND FUTURE WORK}
\label{sec:conclusions}

AmbiScaper is a tool designed for easy dataset creation and exchange, in the context of reverberant Ambisonics Source Localization and Source Separation. It responds to the lack of public datasets for algorithm design, evaluation and reproducibility. An example dataset generated with AmbiScaper, and its analysis with state-of-the-art Source Localization algorithms are also provided.
Further studies in Ambisonics IR position interpolation would allow creating non-static audio scenes, which might be an interesting feature. 
Another feature in progress is the support for a standardized Ambisonics IR format \cite{perez2018ambisonics}, which will ease data reusability and provide the potential to cover a big variety of acoustic scenarios. 







