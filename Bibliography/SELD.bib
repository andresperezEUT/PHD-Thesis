@inproceedings{carpentier2017normalization,
  title={Normalization schemes in Ambisonic: does it matter?},
  author={Carpentier, Thibaut},
  booktitle={Audio Engineering Society Convention 142},
  year={2017},
  organization={Audio Engineering Society}
}

@article{politis2020dataset,
    author = "Politis, Archontis and Adavanne, Sharath and Virtanen, Tuomas",
    title = "A Dataset of Reverberant Spatial Sound Scenes with Moving Sources for Sound Event Localization and Detection",
    year = "2020",
    journal = "arXiv e-prints: 2006.01919",
    eprint = "2006.01919",
    archiveprefix = "arXiv",
    primaryclass = "eess.AS",
    % url = "https://arxiv.org/abs/2006.01919"
}

@article{trowitzsch2019nigens,
  title={The NIGENS General Sound Events Database},
  author={Trowitzsch, Ivo and Taghia, Jalil and Kashef, Youssef and Obermayer, Klaus},
  journal={arXiv preprint arXiv:1902.08314},
  year={2019}
}

@inproceedings {Bertet2006,
author = {Bertet, S. and Daniel, J. and Moreau, S.},
isbn = {9781604235975},
booktitle = {Proc. 120th AES Convention},
pages = {1--24},
title = {{3D Sound Field Recording With Higher Order Ambisonics - Objective Measurements and Validation of a 4th Order Spherical Microphone}},
year = {2006},
month = {May},
address = {Paris, France}
}

@article{Pulkki07,
    author =    {Pulkki, V.},
    title =     {Spatial sound reproduction with directional audio coding},
    journal =   {Journal of the Audio Engineering Society},
    year =      {2007},
    volume =    {55},
    number=     {6},
    pages = 	{503--516},
    month = 	{June}
}

@article{merimaa2005spatial,
  title={Spatial impulse response rendering I: Analysis and synthesis},
  author={Merimaa, Juha and Pulkki, Ville},
  journal={Journal of the Audio Engineering Society},
  volume={53},
  number={12},
  pages={1115--1127},
  month={December},
  year={2005},
  publisher={Audio Engineering Society}
}

@article{epain2016spherical,
  title={Spherical harmonic signal covariance and sound field diffuseness},
  author={Epain, Nicolas and Jin, Craig T},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={10},
  pages={1796--1807},
  month={June},
  year={2016},
  publisher={IEEE}
}


@article{madmoni2018direction,
  title={Direction of arrival estimation for reverberant speech based on enhanced decomposition of the direct sound},
  author={Madmoni, Lior and Rafaely, Boaz},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={13},
  number={1},
  pages={131--142},
  year={2018},
  publisher={IEEE}
}

@inproceedings{sarkka2004rao,
  title={Rao-Blackwellized Monte Carlo data association for multiple target tracking},
  author={S{\"a}rkk{\"a}, Simo and Vehtari, Aki and Lampinen, Jouko},
  booktitle={Proceedings of the Seventh International Conference on Information Fusion},
  volume={1},
  pages={583--590},
  month = {June},
  year={2004},
  address= {Stockholm, Sweden}
}

@article{Adavanne2018_JSTSP,
    author = "Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    title = "Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks",
    year = "2018",
    volume = "13",
    number = "1",
    pages = "34-48",
    doi = "10.1109/JSTSP.2018.2885636",
    issn = "1932-4553",
    month = "March",
    % url = "https://ieeexplore.ieee.org/abstract/document/8567942"
}

@article{adavanne2019localization,
  title={Localization, Detection and Tracking of Multiple Moving Sound Sources with a Convolutional Recurrent Neural Network},
  author={Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
  journal={arXiv preprint arXiv:1904.12769},
  year={2019}
}

@inproceedings{Mesaros_2019_WASPAA,
    author = "Mesaros, Annamaria and Adavanne, Sharath and Politis, Archontis and Heittola, Toni and Virtanen, Tuomas",
    language = "English",
    title = "Joint Measurement of Localization and Detection of Sound Events",
    year = "2019",
    month = "October",
    address = "New Paltz, NY, USA",
    keywords = "Sound event detection and localization, performance evaluation",
    booktitle = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)"
}

@phdthesis{daniel2000representation,
  author       = {Daniel, J{\'e}r{\^o}me}, 
  title        = {Repr{\'e}sentation de champs acoustiques, application {\`a} la transmission et {\`a} la reproduction de sc{\`e}nes sonores complexes dans un contexte multim{\'e}dia},
  school       = {University of Paris VI},
  year         = 2000,
}

@article{xgboost,
  author       = {Chen,Tianqi and Guestrin,Carlos}, 
  title        = {XGBoost, a scalable tree boosting system},
  school       = {University of Washington}
  year         = 2016,
}

@inproceedings{bogdanov2013essentia,
  title={Essentia: An audio analysis library for music information retrieval},
  author={Bogdanov, Dmitry and Wack, Nicolas and G{\'o}mez Guti{\'e}rrez, Emilia and Gulati, Sankalp and Boyer, Herrera and Mayor, Oscar and Roma Trepat, Gerard and Salamon, Justin and Zapata Gonz{\'a}lez, Jos{\'e} Ricardo and Serra, Xavier and others},
  booktitle={14th Conference of the International Society for Music Information Retrieval (ISMIR); p. 493-8.},
  month={November},
  year={2013},
  address= {Curitiba, Brazil}
 
  organization={International Society for Music Information Retrieval (ISMIR)}
}

@article{epain2016spherical,
  title={Spherical harmonic signal covariance and sound field diffuseness},
  author={Epain, Nicolas and Jin, Craig T},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={10},
  pages={1796--1807},
  year={2016},
  publisher={IEEE}
}

@book{rafaely2015fundamentals,
  title={Fundamentals of spherical array processing},
  author={Rafaely, Boaz},
  volume={8},
  year={2015},
  publisher={Springer}
}

@inproceedings{bryan2020impulse,
  title={Impulse Response Data Augmentation and Deep Neural Networks for Blind Room Acoustic Parameter Estimation},
  author={Bryan, Nicholas J},
  booktitle={Proc. IEEE ICASSP},
  month={May},
  year={2020},
  organization={IEEE},
  address = {Barcelona, Spain}
}


@inproceedings{perez2020python,
  title={A Python library for Multichannel Acoustic Signal Processing},
  author={P{\'e}rez-L{\'o}pez, Andr{\'e}s and Politis, Archontis},
  booktitle={Proc. 148th Audio Engineering Society Convention},
  month={May},
  year={2020},
  organization={Audio Engineering Society}
  address={Vienna, Austria}
}

@article{kapka2019sound,
  title={Sound source detection, localization and classification using consecutive ensemble of CRNN models},
  author={Kapka, S{\l}awomir and Lewandowski, Mateusz},
  journal={arXiv preprint arXiv:1908.00766},
  year={2019}
}

@inproceedings{Cao2019,
    author = "Cao, Yin and Kong, Qiuqiang and Iqbal, Turab and An, Fengyan and Wang, Wenwu and Plumbley, Mark",
    title = "Polyphonic Sound Event Detection and Localization using a Two-Stage Strategy",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    address = "New York University, NY, USA",
    month = "October",
    year = "2019",
    pages = "30--34",
}

@inproceedings{grondin2019sound,
  title={Sound Event Localization and Detection Using CRNN on Pairs of Microphones},
  author={Grondin, Fran{\c{c}}ois and Glass, Fran{\c{c}}ois and Sobieraj, Iwona and Plumbley, Mark D},
  booktitle={Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  pages={84--88},
      month = "October",
  year={2019},
address = "New York University, NY, USA",
}

@inproceedings{nguyen2020sequence,
  title={A Sequence Matching Network for Polyphonic Sound Event Localization and Detection},
  author={Nguyen, Thi Ngoc Tho and Jones, Douglas L and Gan, Woon-Seng},
  booktitle={Proc. IEEE ICASSP},
  pages={71--75},
  month={May},
  year={2020},
  organization={IEEE}
  address = "Barcelona, Spain",
}

@inproceedings{tho2014robust,
  title={Robust DOA estimation of multiple speech sources},
  author={Tho, Nguyen Thi Ngoc and Zhao, Shengkui and Jones, Douglas L},
  booktitle={Proc. IEEE ICASSP},
  pages={2287--2291},
    month={May},
  year={2014},
  organization={IEEE},
    address = "Florence, Italy",
}

@inproceedings{perez2019hybrid,
  title={A hybrid parametric-deep learning approach for sound event localization and detection},
  author={P{\'e}rez-L{\'o}pez, Andr{\'e}s and Fonseca, Eduardo and Serra, Xavier},
  booktitle={Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
     month = "October",
  year={2019},
address = "New York University, NY, USA",
    pages={189--193},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 month={November},
 year={2011}
}


@misc{dcasetask,
    title={Sound Event Localization and Detection, Task Description},
    Year={2020},
    note={Accessed on July 2th, 2020},
    author={DCASE},
    howpublished={\url{http://dcase.community/challenge2020/task-sound-event-localization-and-detection-results}}
}

@techreport{Cao2020_task3_report,
    Author = "Cao, Yin and Iqbal, Turab and Kong, Qiuqiang and Yue, Zhong and Wang, Wenwu and Mark, Plumbley",
    title = "EVENT-INDEPENDENT NETWORK FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "Polyphonic sound event localization and detection is to not only detect what sound events are happening but to localize corresponding sound sources. This series of tasks was firstly introduced in DCASE 2019 Task 3. This year, the sound event localization and detection task brings additional challenges in moving sources and up to two overlapping sound events, which include cases of two same type of events with two different direction-of-arrival (DoA) angles. In this report, a novel event-independent network for polyphonic sound event localization and detection is proposed. Unlike the two-stage method that was proposed by us last year [1], this new network is fully end-to-end. Inputs to the network are first-order Ambisonics (FOA) time-domain signals, which are then fed into a 1-D convolutional layer to extract logmel spectrograms and intensity vectors. The network is then split into two parallel branches. The first branch is for the sound event detection (SED), and the second branch is for the DoA estimation. There are three types of predictions from the network, which are SED predictions, event activity detection (EAD) predictions that are used to combine the SED and DOA features for the on-set and off-set estimation, and DoA predictions. All of these predictions have the format of two tracks indicating that there are at most two overlapping events. Within each track, there could be at most one event happening. This architecture brings a problem of track permutation. To address this problem, a frame-level permutation invariant training method is used. Experimental results show that the proposed method can detect polyphonic sound events and their corresponding DoAs. The performance of Task 3 dataset is greatly increased compared with the baseline method."
}

@techreport{Du2020_task3_report,
    Author = "Wang, Qing and Wu, Huaxin and Jing, Zijun and Ma, Feng and Fang, Yi and Wang, Yuxuan and Chen, Tairan and Pan, Jia and Du, Jun and Lee, Chin-Hui",
    title = "THE USTC-IFLYTEK SYSTEM FOR SOUND EVENT LOCALIZATION AND DETECTION OF DCASE2020 CHALLENGE",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "In this report, we present our method for DCASE 2020 challenge: Sound Event Localization and Detection (SELD). We propose an entire technical solution, which consists of data augmentation, network training, model ensemble, and post-processing. First, more training data is generated by applying transformation to both Ambisonic and microphone array signals, and by mixing the non- overlapping samples in the development dataset. And SpecAugment is also used as an augmentation technique to expand the training dataset. Then we train several deep neural network (DNN) architectures to jointly predict the spatial and temporal location of sound events in addition to its type. Besides, for SED estimation, we also use softmax activation function to handle the classification of both non-overlapping and overlapping sound events. With several network architectures, a more robust prediction of SED and directions-of-arrival (DOA) is obtained by model ensemble. At last, we use post-processing to apply different thresholds to different sound events. The proposed system is evaluated on the development set of TAU-NIGENS Spatial Sound Events 2020."
}

@techreport{Naranjo-Alcazar2020_task3_report,
    Author = "Naranjo-Alcazar, Javier and Perez-Castanos, Sergi and Ferrandis, Jose and Zuccarello, Pedro and Cobos, Maximo",
    title = "TASK 3 DCASE 2020: SOUND EVENT LOCALIZATION AND DETECTION USING RESIDUAL SQUEEZE-EXCITATION CNNS",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "Sound Event Localization and Detection (SELD) is a problem related to the field of machine listening whose objective is to recognize individual sound events, detect their temporal activity, and estimate their spatial location. Thanks to the emergence of more hard-labeled audio datasets, Deep Learning techniques have become state-of-the-art solutions. The most common ones are those that implement a convolutional recurrent network (CRNN) having previously transformed the audio signal into multichannel 2D representation. In the context of this problem, the input to the network, usually, has many more channels than in other problems related to machine listening. This is because the audio is recorded by an array of microphones.Some frequency representation is obtained for each of them together with some additional representations, such as the generalized cross-correlation (GCC), whose objective is the assessment of the relationship between channels. This work aims to improve the accuracy results of the baseline CRNN by adding residual squeeze-excitation (SE) blocks in the convolutional part of the CRNN. The followed procedure involves a grid search of the parameter ratio of the residual SE block, whereas the hyperparameters of the network remain the same as in the baseline. Experiments show that by simply introducing the residual SE blocks, the results obtained in the development phase clearly exceed the baseline."
}

@techreport{Nguyen2020_task3_report,
    Author = "Nguyen, Thi Ngoc Tho and Jones, Douglas L. and Gan, Woon Seng",
    title = "DCASE 2020 TASK 3: ENSEMBLE OF SEQUENCE MATCHING NETWORKS FOR DYNAMIC SOUND EVENT LOCALIZATION, DETECTION, AND TRACKING",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "Sound event localization and detection consisted two subtasks which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different event classes, direction-of-arrival estimation uses magnitude or phase differences between microphones to estimate source directions. Therefore, it is often difficult to jointly train two subtasks simultaneously. Our previous sequence matching approach that solves sound event detection and direction-of-arrival separately and trains a convolutional recurrent neural network to associate the sound classes with the directions-of-arrival using onsets and offsets of the sound events shows improved performance for multiple-static-sound-source scenarios compared to other state-of-the-art networks such as the SELDnet, and the two-stage networks. Experimental results on the new DCASE dataset for sound event localization, detection, and tracking of multiple moving sound sources showed that the sequence matching network also outperformed the jointly trained SELDnet model. In order to estimate directions-of-arrival of moving sound sources with high spatial resolution, we proposed to separate the directional estimations into azimuth and elevation before feeding them into the sequence matching network. We combined several sequence matching networks into ensembles and achieved a sound event detection and localization error of 0.217 compared to 0.466 of the baseline."
}

@techreport{Park2020_task3_report,
    Author = "Park, Sooyoung and Suh, Sangwon and Jeong, Youngho",
    title = "SOUND EVENT LOCALIZATION AND DETECTION WITH VARIOUS LOSS FUNCTIONS",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "This technical report presents our system submitted to DCASE 2020 task 3. The goal of DCASE Task 3 is to detect a sound event and its location when a polyphonic sound event moves dynamically. We focus on designing loss functions to overcome the characteristics of the sub-task and imbalanced dataset. Temporal masking loss is used to overcome imbalance from zero labels of the silence frame. Soft floss is used for overcoming imbalance instances between class labels. A periodic loss function is proposed for regression that infers the periodic label in the direction of arrival estimation. Also, we take a feature pyramid network based network to overcome the information leakage occurred by the pooling layer in the CRNN."
}

@techreport{Patel2020_task3_report,
    Author = "Patel, Sohel and Zawodniok, Maciej and Benesty, Jacob",
    title = "DCASE 2020 TASK 3: A SINGLE STAGE FULLY CONVOLUTIONAL NEURAL NETWORK FOR SOUND SOURCE LOCALIZATION AND DETECTION",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "In this report, we present our approach for DCASE 2020 Challenge Task3: Sound event localization and detection. We use a single step training method using SELDNet like models but using fully convolutional architectures. We consider the joint optimization of both event detection and DoA estimation. For the metrics that evaluate the performance of the model consider interdependence of both parameters performance unlike independent performance like DCASE 2019 challenge. We use all the sound event classes and corresponding cartesian co-ordinates for each class to create an image like label for reference and make this an image to image mapping problem. The best model could get DOA error of around 13.5° and error rate of 0.55."
}

@techreport{PerezLopez2020_task3_report,
    Author = "Perez-Lopez, Andres and Ibanez-Usach, Rafael",
    title = "PAPAFIL: A LOW COMPLEXITY SOUND EVENT LOCALIZATION AND DETECTION METHOD WITH PARAMETRIC PARTICLE FILTERING AND GRADIENT BOOSTING",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "The present technical report describes the architecture of the system submitted to the DCASE 2020 Challenge - Task 3: Sound Event Localization and Detection. The proposed method conforms a low complexity solution for the task. It is based on four building blocks: a spatial parametric analysis to find single-source spectrogram bins, a particle tracker to estimate trajectories and temporal activities, a spatial filter, and a gradient boosting machine single-class classifier. Provisional results, computed from the development dataset, show that the proposed method outperforms a CRNN baseline in three out of the four evaluation metrics considered in the challenge, and obtains an overall score almost ten points above the baseline."
}


@techreport{Phan2020_task3_report,
    Author = "Phan, Huy and Pham, Lam and Koch, Philipp and Duong, Ngoc and McLoughlin, Ian and Mertins, Alfred",
    title = "AUDIO EVENT DETECTION AND LOCALIZATION WITH MULTITASK REGRESSION NETWORK",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "This technical report describes our submission to the DCASE 2020 Task 3 (Sound Event Localization and Detection (SELD)). In the submission, we propose a multitask regression model, in which both (multi-label) event detection and localization are formulated as regression problems to use the mean squared error loss homogeneously for model training. The deep learning model features a recurrent convolutional neural network (CRNN) architecture coupled with self-attention mechanism. Experiments on the development set of the challenge's SELD task demonstrate that the proposed system outperforms the DCASE 2020 SELD baseline across all the detection and localization metrics, reducing the overall SELD error (the combined metric) approximately 10\% absolute."
}

@techreport{Politis2020_task3_report,
    Author = "Politis, Archontis and Adavanne, Sharath and Virtanen, Tuomas",
    title = "A DATASET OF REVERBERANT SPATIAL SOUND SCENES WITH MOVING SOURCES FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "This report presents the dataset and the evaluation setup of the Sound Event Localization \& Detection (SELD) task for the DCASE 2020 Challenge. The SELD task refers to the problem of trying to simultaneously classify a known set of sound event classes, detect their temporal activations, and estimate their spatial directions or locations while they are active. To train and test SELD systems, datasets of diverse sound events occurring under realistic acoustic conditions are needed. Compared to the previous challenge, a significantly more complex dataset was created for DCASE 2020. The two key differences are a more diverse range of acoustical conditions, and dynamic conditions, i.e. moving sources. The spatial sound scenes are created using real room impulse responses captured in a continuous manner with a slowly moving excitation source. Both static and moving sound events are synthesized from them. Ambient noise recorded on location is added to complete the generation of scene recordings. A baseline SELD method accompanies the dataset, based on a convolutional recurrent neural network, to provide benchmark scores for the task. The baseline is an updated version of the one used in the previous challenge, with input features and training modifications to improve its performance."
}

@techreport{Ronchini2020_task3_report,
    Author = "Ronchini, Francesca and López, Andrés Pérez and Arteaga, Daniel",
    title = "SOUND EVENT LOCALIZATION AND DETECTION BASED ON CRNN USING DENSE RECTANGULAR FILTERS AND CHANNEL ROTATION DATA AUGMENTATION",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "This technical report illustrates the system submitted to the DCASE 2020 Challenge Task 3: Sound Event Localization and Detection. The algorithm consists of a CRNN using dense rectangular filters specialized to recognize significant frequency features related to the task. In order to further improve the score and to generalize the system performance to unseen data, the training dataset size has been increased using data augmentation based on channel rotations and reflection on the xy plane in the First Order Ambisonic domain, which allow to improve Direction of Arrival labels keeping the physical relationships between channels. Evaluation results on the cross-validation development dataset show that the proposed system outperforms the baseline results, considerably improving Error Rate and F-score for location-aware detection."
}

@techreport{Sampathkumar2020_task3_report,
    Author = "Sampathkumar, Arunodhayan and Kowerko, Danny",
    title = "SOUND EVENT DETECTION AND LOCALIZATION USING CRNN MODELS",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "Sound Event Localization and Detection (SELD) requires both spatial and temporal information of sound events that appears in an acoustic event. The sound event localization and detection DCASE2020 task3 developed a strongly labelled dataset consisting of 14 classes. In this research work the existing method from DCASE2019 is used with significant modifications, where this method utilizes logmel features for sound event detection, and uses intensity vector and generalized cross-correlation (GCC) GCC-PHAT features for sound source localization. The Convolutional Recurrent Neural Network (CRNN) is developed that jointly predicts the Sound Event Detection (SED) and Degree of Arrival (DOA) hence minimizing the overlapping problems. The developed model significantly outperformed the baseline system."
}

@techreport{Shimada2020_task3_report,
    Author = "Shimada, Kazuki and Takahashi, Naoya and Takahashi, Shusuke and Mitsufuji, Yuki",
    title = "SOUND EVENT LOCALIZATION AND DETECTION USING ACTIVITY-COUPLED CARTESIAN DOA VECTOR AND RD3NET",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "Our systems submitted to the DCASE2020 task 3: Sound Event Localization and Detection (SELD) are described in this report. We consider two systems: a single-stage system that solve sound event localization (SEL) and sound event detection (SED) simultaneously, and a two-stage system that first handles the SED and SEL tasks individually and later combines those results. As the single-stage system, we propose a unified training framework that uses an activity-coupled Cartesian DOA vector (ACCDOA) representation as a single target for both the SED and SEL tasks. To efficiently estimate sound event locations and activities, we further propose RD3Net, which incorporates recurrent and convolution layers with dense skip connections and dilation. To generalize the models, we apply three data augmentation techniques: equalized mixture data augmentation (EMDA), rotation of first-order Ambisonic (FOA) singals, and multichannel extension of SpecAugment. Our systems demonstrate a significant improvement over the baseline system."
}

@techreport{Singla2020_task3_report,
    Author = "Singla, Rohit and Tiwari, Sourabh and Sharma, Rajat",
    title = "A SEQUENTIAL SYSTEM FOR SOUND EVENT DETECTION AND LOCALIZATION USING CRNN",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "In this technical report, we describe our method for DCASE2020 task 3: Sound Event Localization and Detection. We use a CRNN SELDnet-like single output models which run on the features extracted from audio files using log-mel spectrogram. Our model uses CNN layers followed by RNN layers followed by predicting sound event classes: Sound Event Detection (SED) and then giving the output of SED to estimate Direction Of Arrival (DOA) for those sound events and then the final output is given as a concatenation of SED and DOA. The proposed approach is evaluated on the development set of TAU Spatial Sound Events 2020 - First-Order Ambisonics (FOA)."
}

@techreport{Song2020_task3_report,
    Author = "Song, Ju-man",
    title = "LOCALIZATION AND DETECTION FOR MOVING SOUND SOURCES USING CONSECUTIVE ENSEMBLE OF 2D-CRNN",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "July",
    abstract = "This technical report introduces a deep learning strategy for sound event localization and detection in DCASE 2020 Task 3. This strategy is designed to get accurate estimation of both detecting and localizing moving sound events by splitting a task into five sub-tasks. Each subtask estimates the number of existing sound sources, the number of sound directions, single sound direction, multiple sound directions, and category of events. Thus, each two dimensional convolutional recurrent neural network (2D-CRNN) is focused on each sub-task. In this way, we could improve its robustness to complex conditions. Finally, the consecutive ensemble strategy is performed to achieve high performance with some decision logic. With the proposed strategy, we could get optimal network models for each sub-task. The proposed strategy is evaluated on the development set of TAU-NIGENS Spatial Sound Events 2020, and shows notable improvements."
}

@techreport{Tian2020_task3_report,
    Author = "Tian, Congzhou",
    title = "MULTIPLE CRNN FOR SELD",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this task, we use multiple CRNN for SELD. Firstly, there is a CRNN to predict the number of sound events at the same time. A SED CRNN is used to predict the current sound events given the activated number result. After that, we train a DOA1 CRNN specifically for frames with single active event and a total DOA CRNN for frames with more active events. We think training with separate network is helpful for both SED and DOA tasks and our results are proved better than the baseline method on the development dataset."
}
